# -*- coding: utf-8 -*-
"""nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dfBTZS5tNauCwFBrDQLVlHJZ49N6kk-M
"""



# Cell: Text Classification into Custom Categories

from transformers import pipeline

# Load a zero-shot-classification pipeline
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# Define your candidate labels
labels = ["food waste", "nature", "waste management"]

# Example text (you can replace this with your own)
text = "Composting organic leftovers helps reduce food waste and improve soil health."

# Perform classification
result = classifier(text, candidate_labels=labels)

# Display result
print("Input Text:", text)
print("Predicted Category:", result['labels'][0])
print("Scores:", dict(zip(result['labels'], result['scores'])))

# prompt: # Cell: Text Classification into Custom Categories
# from transformers import pipeline
# # Load a zero-shot-classification pipeline
# classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
# # Define your candidate labels
# labels = ["food waste", "nature", "waste management"]
# # Example text (you can replace this with your own)
# text = "Composting organic leftovers helps reduce food waste and improve soil health."
# # Perform classification
# result = classifier(text, candidate_labels=labels)
# # Display result
# print("Input Text:", text)
# print("Predicted Category:", result['labels'][0])
# print("Scores:", dict(zip(result['labels'], result['scores'])))
# create a gardio

!pip install -q gradio
import gradio as gr

# Define the prediction function
def predict_category(text):
  result = classifier(text, candidate_labels=labels)
  # Return the predicted category and a dictionary of scores
  return result['labels'][0], dict(zip(result['labels'], result['scores']))

# Create the Gradio interface
iface = gr.Interface(
    fn=predict_category,
    inputs="text",
    outputs=["text", "json"], # Output the predicted category as text and scores as json
    title="Text Classification",
    description="Enter text and get the predicted category and scores."
)

# Launch the interface
iface.launch(debug=True)

from transformers import AutoProcessor, AutoTokenizer, AutoModelForCausalLM
from diffusers import StableDiffusionPipeline
import torch
import matplotlib.pyplot as plt

# Load the model (Stable Diffusion via Hugging Face)
pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
pipe = pipe.to("cuda" if torch.cuda.is_available() else "cpu")

# Input text prompt
prompt = "A futuristic city at sunset with flying cars"

# Generate image
image = pipe(prompt).images[0]

# Display image
plt.imshow(image)
plt.axis("off")
plt.title(prompt)
plt.show()

# prompt: from transformers import AutoProcessor, AutoTokenizer, AutoModelForCausalLM
# from diffusers import StableDiffusionPipeline
# import torch
# import matplotlib.pyplot as plt
# # Load the model (Stable Diffusion via Hugging Face)
# pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
# pipe = pipe.to("cuda" if torch.cuda.is_available() else "cpu")
# # Input text prompt
# prompt = "A futuristic city at sunset with flying cars"
# # Generate image
# image = pipe(prompt).images[0]
# # Display image
# plt.imshow(image)
# plt.axis("off")
# plt.title(prompt)
# plt.show()
# create a gardio

!pip install gradio -q

import gradio as gr
import torch
from diffusers import StableDiffusionPipeline

# Load the model (Stable Diffusion via Hugging Face)
pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
pipe = pipe.to("cuda" if torch.cuda.is_available() else "cpu")

def generate_image(prompt):
  """Generates an image based on a text prompt using Stable Diffusion."""
  image = pipe(prompt).images[0]
  return image

# Create the Gradio interface
iface = gr.Interface(
    fn=generate_image,
    inputs=gr.Textbox(label="Enter text prompt"),
    outputs=gr.Image(label="Generated Image"),
    title="Text-to-Image "
)

# Launch the interface
iface.launch()

from transformers import pipeline
import matplotlib.pyplot as plt
from collections import Counter

# Load NER pipeline
ner = pipeline("ner", grouped_entities=True)

# Input text
text = "BTS performed at Wembley Stadium in London, and their leader RM gave a speech at the United Nations in New York."

# Perform NER
ner_results = ner(text)

# Count entity labels
entity_counts = Counter([entity['entity_group'] for entity in ner_results])

# Plotting the NER results
plt.figure(figsize=(8, 5))
plt.bar(entity_counts.keys(), entity_counts.values(), color='skyblue')
plt.title("Named Entities Found")
plt.xlabel("Entity Type")
plt.ylabel("Count")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Print detailed entities
for entity in ner_results:
    print(f"{entity['word']} → {entity['entity_group']} ({entity['score']:.2f})")

# prompt: from transformers import pipeline
# import matplotlib.pyplot as plt
# from collections import Counter
# # Load NER pipeline
# ner = pipeline("ner", grouped_entities=True)
# # Input text
# text = "BTS performed at Wembley Stadium in London, and their leader RM gave a speech at the United Nations in New York."
# # Perform NER
# ner_results = ner(text)
# # Count entity labels
# entity_counts = Counter([entity['entity_group'] for entity in ner_results])
# # Plotting the NER results
# plt.figure(figsize=(8, 5))
# plt.bar(entity_counts.keys(), entity_counts.values(), color='skyblue')
# plt.title("Named Entities Found")
# plt.xlabel("Entity Type")
# plt.ylabel("Count")
# plt.grid(axis='y', linestyle='--', alpha=0.7)
# plt.show()
# # Print detailed entities
# for entity in ner_results:
#     print(f"{entity['word']} → {entity['entity_group']} ({entity['score']:.2f})")
# create a gardio

!pip install -q gradio

import gradio as gr

def analyze_entities(text):
  """Performs NER and returns a string representation of the results."""
  ner_results = ner(text)
  output_string = ""
  for entity in ner_results:
    output_string += f"{entity['word']} → {entity['entity_group']} ({entity['score']:.2f})\n"
  return output_string

# Create the Gradio interface
iface_ner = gr.Interface(
    fn=analyze_entities,
    inputs=gr.Textbox(label="Enter text for NER analysis"),
    outputs=gr.Textbox(label="Named Entities"),
    title="Named Entity Recognition (NER)",
    description="Enter text to identify and classify named entities."
)

# Launch the interface
iface_ner.launch()

from transformers import pipeline

# Load fill-mask pipeline
fill_mask = pipeline("fill-mask", model="bert-base-uncased")

# Masked sentence
masked_text = "The Eiffel Tower is located in [MASK]."

# Get top predictions
results = fill_mask(masked_text)

# Display predictions
for res in results:
    print(f"{res['sequence']}  (score: {res['score']:.4f})")

# prompt: from transformers import pipeline
# # Load fill-mask pipeline
# fill_mask = pipeline("fill-mask", model="bert-base-uncased")
# # Masked sentence
# masked_text = "The Eiffel Tower is located in [MASK]."
# # Get top predictions
# results = fill_mask(masked_text)
# # Display predictions
# for res in results:
#     print(f"{res['sequence']}  (score: {res['score']:.4f})")
# create a gardio

import gradio as gr

# Define the prediction function
def predict_mask(text):
  """Uses the fill-mask pipeline to predict the masked token."""
  results = fill_mask(text)
  # Format results for display
  output_string = ""
  for res in results:
    output_string += f"{res['sequence']}  (score: {res['score']:.4f})\n"
  return output_string

# Create the Gradio interface
iface_mask = gr.Interface(
    fn=predict_mask,
    inputs=gr.Textbox(label="Enter text with a [MASK] token"),
    outputs=gr.Textbox(label="Top predictions"),
    title="Fill Mask",
    description="Enter a sentence with a [MASK] token to get predictions."
)

# Launch the interface
iface_mask.launch()
